#h) compare test error between pruned and unpruned trees
Yhat.test <- predict(tree.oj, newdata=data.test, type="class")
t <- table(Yhat.test,data.test$Purchase)
miss.test.unpruned <- (t["CH","MM"]+t["MM","CH"])/sum(t)
t
Yhat.test.prune <- predict(final.tree, newdata=data.test, type="class")
t <- table(Yhat.test.prune, data.test$Purchase)
miss.test.pruned <- (t["CH","MM"]+t["MM","CH"])/sum(t)
t
final.tree <- prune.tree(tree.oj, best=8)
plot(final.tree)
text(final.tree,pretty=0)
#g) compare training error between pruned and unpruned trees
Yhat.train <- predict(tree.oj, newdata=data.train,type="class")
t <- table(Yhat.train, data.train$Purchase)
miss.train.unpruned <- (t["CH","MM"]+t["MM","CH"])/sum(t)
Yhat.train.prune <- predict(final.tree, newdata=data.train, type="class")
t <- table(Yhat.train.prune, data.train$Purchase)
miss.train.pruned <- (t["CH","MM"]+t["MM","CH"])/sum(t)
miss.train.unpruned
miss.train.pruned
Yhat.test <- predict(tree.oj, newdata=data.test, type="class")
t <- table(Yhat.test,data.test$Purchase)
miss.test.unpruned <- (t["CH","MM"]+t["MM","CH"])/sum(t)
Yhat.test.prune <- predict(final.tree, newdata=data.test, type="class")
t <- table(Yhat.test.prune, data.test$Purchase)
miss.test.pruned <- (t["CH","MM"]+t["MM","CH"])/sum(t)
miss.test.unpruned
miss.test.pruned
#e)
result <- cv.tree(tree.oj, FUN=prune.tree, K=10)
plot(result)
final.tree <- prune.tree(tree.oj, best=9)
plot(final.tree)
text(final.tree,pretty=0)
#g) compare training error between pruned and unpruned trees
Yhat.train <- predict(tree.oj, newdata=data.train,type="class")
t <- table(Yhat.train, data.train$Purchase)
miss.train.unpruned <- (t["CH","MM"]+t["MM","CH"])/sum(t)
Yhat.train.prune <- predict(final.tree, newdata=data.train, type="class")
t <- table(Yhat.train.prune, data.train$Purchase)
miss.train.pruned <- (t["CH","MM"]+t["MM","CH"])/sum(t)
miss.train.unpruned
miss.train.pruned
Yhat.test <- predict(tree.oj, newdata=data.test, type="class")
t <- table(Yhat.test,data.test$Purchase)
miss.test.unpruned <- (t["CH","MM"]+t["MM","CH"])/sum(t)
Yhat.test.prune <- predict(final.tree, newdata=data.test, type="class")
t <- table(Yhat.test.prune, data.test$Purchase)
miss.test.pruned <- (t["CH","MM"]+t["MM","CH"])/sum(t)
miss.test.unpruned
miss.test.pruned
#e)
result <- cv.tree(tree.oj, FUN=prune.tree, K=10)
plot(result)
final.tree <- prune.tree(tree.oj, best=5)
plot(final.tree)
text(final.tree,pretty=0)
#g) compare training error between pruned and unpruned trees
Yhat.train <- predict(tree.oj, newdata=data.train,type="class")
t <- table(Yhat.train, data.train$Purchase)
miss.train.unpruned <- (t["CH","MM"]+t["MM","CH"])/sum(t)
Yhat.train.prune <- predict(final.tree, newdata=data.train, type="class")
t <- table(Yhat.train.prune, data.train$Purchase)
miss.train.pruned <- (t["CH","MM"]+t["MM","CH"])/sum(t)
miss.train.unpruned
miss.train.pruned
#h) compare test error between pruned and unpruned trees
Yhat.test <- predict(tree.oj, newdata=data.test, type="class")
t <- table(Yhat.test,data.test$Purchase)
miss.test.unpruned <- (t["CH","MM"]+t["MM","CH"])/sum(t)
Yhat.test.prune <- predict(final.tree, newdata=data.test, type="class")
t <- table(Yhat.test.prune, data.test$Purchase)
miss.test.pruned <- (t["CH","MM"]+t["MM","CH"])/sum(t)
miss.test.unpruned
miss.test.pruned
final.tree <- prune.tree(tree.oj, best=8)
plot(final.tree)
text(final.tree,pretty=0)
#g) compare training error between pruned and unpruned trees
Yhat.train <- predict(tree.oj, newdata=data.train,type="class")
t <- table(Yhat.train, data.train$Purchase)
miss.train.unpruned <- (t["CH","MM"]+t["MM","CH"])/sum(t)
Yhat.train.prune <- predict(final.tree, newdata=data.train, type="class")
t <- table(Yhat.train.prune, data.train$Purchase)
miss.train.pruned <- (t["CH","MM"]+t["MM","CH"])/sum(t)
miss.train.unpruned
miss.train.pruned
#h) compare test error between pruned and unpruned trees
Yhat.test <- predict(tree.oj, newdata=data.test, type="class")
t <- table(Yhat.test,data.test$Purchase)
miss.test.unpruned <- (t["CH","MM"]+t["MM","CH"])/sum(t)
Yhat.test.prune <- predict(final.tree, newdata=data.test, type="class")
t <- table(Yhat.test.prune, data.test$Purchase)
miss.test.pruned <- (t["CH","MM"]+t["MM","CH"])/sum(t)
miss.test.unpruned
miss.test.pruned
result <- cv.tree(tree.default, FUN=prune.tree, K=10)
tree.default <- tree(default ~ student+balance+income,data=data.train)
result <- cv.tree(tree.default, FUN=prune.tree, K=10)
plot(result)
tree.default <- tree(default ~ student+balance+income,data=data.train)
#b)
tree.oj <- tree(Purchase~., data=data.train)
summary(tree.oj)
#c)
plot(tree.oj)
text(tree.oj,pretty=0)
Yhat.test <- predict(tree.oj, newdata=data.test,type="class")
t <- table(Yhat.test,data.test$Purchase)
miss.test <- (t["CH","MM"]+t["MM","CH"])/sum(t)
miss.test
#e)
result <- cv.tree(tree.oj, FUN=prune.tree, K=10)
plot(result)
final.tree <- prune.tree(tree.oj, best=6)
plot(final.tree)
text(final.tree,pretty=0)
#g) compare training error between pruned and unpruned trees
Yhat.train <- predict(tree.oj, newdata=data.train,type="class")
t <- table(Yhat.train, data.train$Purchase)
miss.train.unpruned <- (t["CH","MM"]+t["MM","CH"])/sum(t)
Yhat.train.prune <- predict(final.tree, newdata=data.train, type="class")
t <- table(Yhat.train.prune, data.train$Purchase)
miss.train.pruned <- (t["CH","MM"]+t["MM","CH"])/sum(t)
miss.train.unpruned
miss.train.pruned
#h) compare test error between pruned and unpruned trees
Yhat.test <- predict(tree.oj, newdata=data.test, type="class")
t <- table(Yhat.test,data.test$Purchase)
miss.test.unpruned <- (t["CH","MM"]+t["MM","CH"])/sum(t)
Yhat.test.prune <- predict(final.tree, newdata=data.test, type="class")
t <- table(Yhat.test.prune, data.test$Purchase)
miss.test.pruned <- (t["CH","MM"]+t["MM","CH"])/sum(t)
miss.test.unpruned
miss.test.pruned
thresholds <- c(0.1, 0.5, 1, 2,3,4,5,6,7,8,9,10,11,12,13,14,15)
#SVM model
svm.linear <- svm(Purchase~.,data=data.train,type="C-classification",
kernel="linear", cost=1)
#SVM kernel=3 model
svm.poly <- svm(Purchase~.,data=data.train,type="C-classification", kernel='polynomial',
degree=3, cost=1)
#SVM radial kernel, gamma=0.1
svm.rad <- svm(Purchase~.,data=data.train,type="C-classification",kernel='radial',
gamma=0.1,cost=1)
svm.lin.vals <- rep(NA, length(thresholds))
svm.poly.vals <- rep(NA, length(thresholds))
svm.rad.vals <- rep(NA, length(thresholds))
for (i in 1:length(thresholds)) {
svm.linear <- svm(Purchase~.,data=data.train,type="C-classification",
kernel="linear", cost=thresholds[i])
Y.hat.linear <- predict(svm.linear, data.test)
t <- table(Y.hat.linear,data.val$Purchase)
svm.lin.vals[i] <- (t["CH","MM"]+t["MM","CH"])/sum(t)
svm.poly <- svm(Purchase~.,data=data.train,type="C-classification", kernel='polynomial',
degree=3, cost=thresholds[i])
Y.hat.poly <- predict(svm.poly, data.test)
t <- table(Y.hat.poly,data.val$Purchase)
svm.poly.vals[i] <- (t["CH","MM"]+t["MM","CH"])/sum(t)
svm.rad <- svm(Purchase~.,data=data.train,type="C-classification",kernel='radial',
gamma=0.1,cost=thresholds[i])
Y.hat.rad <- predict(svm.rad, data.test)
t <- table(Y.hat.rad,data.val$Purchase)
svm.rad.vals[i] <- (t["CH","MM"]+t["MM","CH"])/sum(t)
}
table2 <- matrix(NA,nrow=17,ncol=3)
table2[,1] <- svm.lin.vals
table2[,2] <- svm.poly.vals
table2[,3] <- svm.rad.vals
rownames(table2) <- thresholds
colnames(table2) <- c("Linear SVM","Polynomial SVM","Radial SVM")
table2
plot(thresholds, svm.iin.vals)
plot(thresholds, svm.lin.vals)
lines(threholds, svm.poly.vals)
lines(thresholds, svm.poly.vals)
lines(thresholds, svm.rad.vals)
plot(thresholds, svm.lin.vals, xlim=c(0,0.3))
plot(thresholds, svm.lin.vals, xlim=c(0.12,0.25))
plot(thresholds, svm.lin.vals, ylim=c(0.12,0.25))
lines(thresholds, svm.poly.vals)
lines(thresholds, svm.rad.vals)
lines(thresholds, svm.lin.vals)
plot(thresholds, svm.lin.vals, ylim=c(0.12,0.25),type='b')
lines(thresholds, svm.poly.vals)
lines(thresholds, svm.rad.vals)
#lines(thresholds, svm.lin.vals)
points(thresholds, svm.poly.vals,type='b')
plot(thresholds, svm.lin.vals, ylim=c(0.12,0.25),type='b')
#lines(thresholds, svm.lin.vals)
points(thresholds, svm.poly.vals,type='b')
points(thresholds, svm.rad.vals,type='b')
help("plot")
help(plot)
plot(thresholds, svm.lin.vals, ylim=c(0.12,0.25),type='c')
plot(thresholds, svm.lin.vals, ylim=c(0.12,0.25),type='b')
type
='
#lines(thresholds, svm.lin.vals)
points(thresholds, svm.poly.vals,type='b')
points(thresholds, svm.rad.vals,type='b')
plot(thresholds, svm.lin.vals, ylim=c(0.12,0.25),type='b')
#lines(thresholds, svm.lin.vals)
points(thresholds, svm.poly.vals,type='b',col='red')
points(thresholds, svm.rad.vals,type='b',col='forest green')
legend(legend=c('Linear SVM','Polynomial SVM','Radial SVM'),
col=c('black','red','forest green'))
legend(13,0.23legend=c('Linear SVM','Polynomial SVM','Radial SVM'),
col=c('black','red','forest green'))
legend(13,0.23,legend=c('Linear SVM','Polynomial SVM','Radial SVM'),
col=c('black','red','forest green'))
legend(11,0.23,legend=c('Linear SVM','Polynomial SVM','Radial SVM'),
col=c('black','red','forest green'))
legend(10,0.238,legend=c('Linear SVM','Polynomial SVM','Radial SVM'),
col=c('black','red','forest green'))
plot(thresholds, svm.lin.vals, ylim=c(0.12,0.25),type='b')
#lines(thresholds, svm.lin.vals)
points(thresholds, svm.poly.vals,type='b',col='red')
points(thresholds, svm.rad.vals,type='b',col='forest green')
legend(10,0.238,legend=c('Linear SVM','Polynomial SVM','Radial SVM'),
col=c('black','red','forest green'))
legend(10,0.238,legend=c('Linear SVM','Polynomial SVM','Radial SVM'),
col=c('black','red','forest green'),
fill=c('black','red','forest green'))
legend(10,0.238,legend=c('Linear SVM','Polynomial SVM','Radial SVM'),
col=c('black','red','forest green'),
fill=c('black','red','forest green'),cex=0.8)
legend(10,0.238,legend=c('Linear SVM','Polynomial SVM','Radial SVM'),
col=c('black','red','forest green'),
fill=c('black','red','forest green'),cex=0.8,pch=2)
legend(10,0.238,legend=c('Linear SVM','Polynomial SVM','Radial SVM'),
col=c('black','red','forest green'),
fill=c('black','red','forest green'),cex=0.8,pch=1)
plot(thresholds, svm.lin.vals, ylim=c(0.12,0.25),type='b')
#lines(thresholds, svm.lin.vals)
points(thresholds, svm.poly.vals,type='b',col='red')
points(thresholds, svm.rad.vals,type='b',col='forest green')
legend(10,0.238,legend=c('Linear SVM','Polynomial SVM','Radial SVM'),
col=c('black','red','forest green'),
fill=c('black','red','forest green'),cex=0.8,pch=1)
legend(9,0.238,legend=c('Linear SVM','Polynomial SVM','Radial SVM'),
col=c('black','red','forest green'),
fill=c('black','red','forest green'),cex=0.7,pch=1)
plot(thresholds, svm.lin.vals, ylim=c(0.12,0.25),type='b')
#lines(thresholds, svm.lin.vals)
points(thresholds, svm.poly.vals,type='b',col='red')
points(thresholds, svm.rad.vals,type='b',col='forest green')
legend(9,0.238,legend=c('Linear SVM','Polynomial SVM','Radial SVM'),
col=c('black','red','forest green'),
fill=c('black','red','forest green'),cex=0.7,pch=1)
legend(9,0.238,legend=c('Linear SVM','Polynomial SVM','Radial SVM'),
col=c('black','red','forest green'),
fill=c('black','red','forest green'),cex=0.7)
legend(9.5,0.24,legend=c('Linear SVM','Polynomial SVM','Radial SVM'),
col=c('black','red','forest green'),
fill=c('black','red','forest green'),cex=0.75)
plot(thresholds, svm.lin.vals, ylim=c(0.12,0.25),type='b')
points(thresholds, svm.poly.vals,type='b',col='red')
points(thresholds, svm.rad.vals,type='b',col='forest green')
legend(9.5,0.24,legend=c('Linear SVM','Polynomial SVM','Radial SVM'),
col=c('black','red','forest green'),
fill=c('black','red','forest green'),cex=0.75)
points(0.1,0)
plot(thresholds, svm.lin.vals, ylim=c(0,0.25),type='b')
points(thresholds, svm.poly.vals,type='b',col='red')
points(thresholds, svm.rad.vals,type='b',col='forest green')
points(0.1,0)
legend(9.5,0.24,legend=c('Linear SVM','Polynomial SVM','Radial SVM'),
col=c('black','red','forest green'),
fill=c('black','red','forest green'),cex=0.75)
points(0.12,0)
plot(thresholds, svm.lin.vals, ylim=c(0.12,0.25),type='b')
points(thresholds, svm.poly.vals,type='b',col='red')
points(thresholds, svm.rad.vals,type='b',col='forest green')
points(0.12,0)
legend(9.5,0.24,legend=c('Linear SVM','Polynomial SVM','Radial SVM'),
col=c('black','red','forest green'),
fill=c('black','red','forest green'),cex=0.75)
points(0.1,0.12)
points(c(0.1,0.5),c(0.12,0.12)
points(c(0.1,0.5),c(0.12,0.12))
plot(thresholds, svm.lin.vals, ylim=c(0.12,0.26),type='b')
points(thresholds, svm.poly.vals,type='b',col='red')
table2
plot(thresholds, svm.lin.vals, ylim=c(0.12,0.274),type='b')
points(thresholds, svm.poly.vals,type='b',col='red')
points(thresholds, svm.rad.vals,type='b',col='forest green')
legend(9.5,0.24,legend=c('Linear SVM','Polynomial SVM','Radial SVM'),
col=c('black','red','forest green'),
fill=c('black','red','forest green'),cex=0.75)
legend(9.5,0.26,legend=c('Linear SVM','Polynomial SVM','Radial SVM'),
col=c('black','red','forest green'),
fill=c('black','red','forest green'),cex=0.75)
plot(thresholds, svm.lin.vals, ylim=c(0.12,0.274),type='b')
points(thresholds, svm.poly.vals,type='b',col='red')
points(thresholds, svm.rad.vals,type='b',col='forest green')
legend(9.5,0.26,legend=c('Linear SVM','Polynomial SVM','Radial SVM'),
col=c('black','red','forest green'),
fill=c('black','red','forest green'),cex=0.75)
knitr::opts_chunk$set(echo = TRUE)
library('ISLR')
attach(OJ)
library("tree")
library('MASS')
library('e1071')
summary(tree.oj)
names(data.train)
plot(tree.oj)
text(tree.oj,pretty=0)
help(OJ)
miss.test
Yhat.test <- predict(tree.oj, newdata=data.test,type="class")
t <- table(Yhat.test,data.test$Purchase)
miss.test <- (t["CH","MM"]+t["MM","CH"])/sum(t)
miss.test
result <- cv.tree(tree.oj, FUN=prune.tree, K=10)
plot(result)
result <- cv.tree(tree.oj, FUN=prune.tree, K=10)
plot(result)
result <- cv.tree(tree.oj, FUN=prune.tree, K=10)
plot(result)
result <- cv.tree(tree.oj, FUN=prune.tree, K=10)
plot(result)
miss.logit
miss.LDA
miss.test #from tree above
summary(fit.logit)
table2
plot(thresholds, svm.lin.vals, ylim=c(0.12,0.274),type='b')
points(thresholds, svm.poly.vals,type='b',col='red')
points(thresholds, svm.rad.vals,type='b',col='forest green')
legend(9.5,0.26,legend=c('Linear SVM','Polynomial SVM','Radial SVM'),
col=c('black','red','forest green'),
fill=c('black','red','forest green'),cex=0.75)
final.tree <- prune.tree(tree.oj, best=9)
plot(final.tree)
text(final.tree,pretty=0)
Yhat.train <- predict(tree.oj, newdata=data.train,type="class")
t <- table(Yhat.train, data.train$Purchase)
miss.train.unpruned <- (t["CH","MM"]+t["MM","CH"])/sum(t)
Yhat.train.prune <- predict(final.tree, newdata=data.train, type="class")
t <- table(Yhat.train.prune, data.train$Purchase)
miss.train.pruned <- (t["CH","MM"]+t["MM","CH"])/sum(t)
miss.train.unpruned
miss.train.pruned
setwd("C:/Users/abguh/Desktop/stat228/final project")
df <- read.csv("raw_data.csv",header=TRUE)
nrow(df)
df <- read.csv('fighters.csv')
setwd("C:/Users/abguh/Desktop/stat228")
fighters.csv
df <- read.csv('fighters.csv')
kmeans.obj <- kmeans(df, 10)
df
names(df)
kmeans.obj <- kmeans(df[,c(2,3,4,5)], 10)
kmeans.obj$tot.withinss
for (i in 1:10){
kmeans.obj <- kmeans(df[,c(2,3,4,5)], 10)
sumsquares[i] <- kmeans.obj$tot.withinss
}
sumsquares <- rep(NA, 10)
for (i in 1:10){
kmeans.obj <- kmeans(df[,c(2,3,4,5)], 10)
sumsquares[i] <- kmeans.obj$tot.withinss
}
sumsquares
sumsquares <- rep(NA, 10)
for (i in 1:10){
kmeans.obj <- kmeans(df[,c(2,3,4,5)], i)
sumsquares[i] <- kmeans.obj$tot.withinss
}
sumsquares
plot(seq(1:10,1),sumsquares)
plot(seq(1,10,1),sumsquares)
plot(seq(1,10,1),sumsquares,type='b')
######################################
kmeans.obj <- kmeans(df[,c(2,3,4,5)], 2)
kmeans.obj$cluster
###############################
df.standard <- scale(df[,c(2,3,4,5)])
cluster1 <- kmeans.obj$cluster
kmeans.obj2 <- kmeans(df.standard, 2)
cluster2 <- kmeans.obj2$cluster
cluster1
cluster2
df.standard
plot(df$SPR, df$RGF)
plot(df$SPR, df$RGF,col=kmeans.obj=cluster1)
plot(df$SPR, df$RGF,col=cluster1)
plot(df$SPR, df$RGF,col=cluster1,cex=2)
points(df$SPR, df$RGF, col=cluster2, cex=0.5)
cluster2 <- ifelse(cluster2==1,2, 1)
plot(df$SPR, df$RGF,col=cluster1,cex=2)
points(df$SPR, df$RGF, col=cluster2, cex=0.5)
plot(df$SPR, df$PLF,col=cluster1,cex=2)
points(df$SPR, df$PLF, col=cluster2, cex=0.5)
plot(df$SPR, df$SLF,col=cluster1,cex=2)
points(df$SPR, df$SLF, col=cluster2, cex=0.5)
points(df$SPR, df$SLF, col=df$CAR,pch=2)
df$CAR
points(df$SPR, df$SLF, col=ifelse(df$CAR=='no','green','red'), pch=2)
points(df$SPR, df$SLF, pch=ifelse(df$CAR=='no',2,1)) #, pch=2)
plot(df$SPR, df$SLF,col=cluster1,cex=2)
points(df$SPR, df$SLF, col=cluster2, cex=0.5)
cluster1
cluster2
##################################
points(df$SPR, df$SLF, pch=ifelse(df$CAR=='no',2,1)) #, pch=2)
ifelse
points(df$SPR, df$RGF, pch=ifelse(df$CAR=='no',2,1)) #, pch=2)
plot(df$SPR, df$SLF,col=cluster1,cex=2)
points(df$SPR, df$SLF, col=cluster2, cex=0.5)
cluster1
cluster2
##################################
points(df$SPR, df$RGF, pch=ifelse(df$CAR=='no',2,1)) #, pch=2)
plot(df$SPR, df$SLF,col=cluster1,cex=2)
points(df$SPR, df$SLF, col=cluster2, cex=0.5)
cluster1
cluster2
##################################
points(df$SPR, df$SLF, cex=ifelse(df$CAR=='no',2,1), pch=2)
plot(df$SPR, df$SLF,col=cluster1,cex=2)
points(df$SPR, df$SLF, col=cluster2, cex=0.5)
cluster1
cluster2
##################################
points(df$SPR, df$SLF, cex=ifelse(df$CAR=='no',2,1))
names(df)
pairs(df[,c(2,3,4,5,6)])
pairs(df[,c(2,3,4,5)])
pairs(df[,c(2,3,4,5)],col=cluster1)
pairs(df[,c(2,3,4,5)],col=df$CAR)
pairs(df[,c(2,3,4,5)],col=ifelse(df$CAR=='no','green','red'))
plot(df$SPR, df$SLF,col=cluster1,cex=2)
points(df$SPR, df$SLF, col=cluster2, cex=0.5)
cluster1
cluster2
##################################
points(df$SPR, df$SLF, cex=ifelse(df$CAR=='no',2,1))
df.noout$percen_gen
setwd("C:/Users/abguh/Desktop/cred lab/Mayoral Project/capstone22")
df <- read.csv("../Untitled Folder/model_data.csv", header = TRUE)
df$percen_gen
logit(df$percen_gen)
log(df$percen_gen/(1-df$percen_gen))
df
####binomial
df$not_gen <- df$num_queries - df$num_gen
glm(df[,"num_gen","not_gen"]~num_queries+num_queries_unique+avg_len+gender,
df=df.noout,family=binomial('logit'))
glm(df[,c("num_gen","not_gen")]~city+num_queries+num_queries_unique+avg_len+gender,
df=df.noout,family=binomial('logit'))
glm(as.matrix(df[,c("num_gen","not_gen")])~city+num_queries+num_queries_unique+avg_len+gender,
df=df.noout,family=binomial('logit'))
df <- read.csv("../Untitled Folder/model_data.csv", header = TRUE)
df$city <- as.factor(df$city)
df$gender <- as.factor(df$gender)
df$name <- as.factor(df$name)
df$race_detailed <- as.factor(df$race_detailed)
#remove outlier
df.noout <- df[-17,]
####binomial
df$not_gen <- df$num_queries - df$num_gen
glm(as.matrix(df[,c("num_gen","not_gen")])~city+num_queries+num_queries_unique+avg_len+gender,
df=df.noout,family=binomial('logit'))
as.matrix(df[,c("num_gen","not_gen")])
glm(as.matrix(df[,c("num_gen","not_gen")])~city+gender,
df=df.noout,family=binomial('logit'))
glm(cbind(df$num_gen,df$not_gen)~city+gender,
df=df.noout,family=binomial('logit'))
glm.obj <- glm(cbind(df$num_gen,df$not_gen)~city+gender,
df=df.noout,family=binomial('logit'))
glm.obj <- glm(cbind(df$num_gen,df$not_gen)~city+gender,
data=df.noout,family=binomial('logit'))
####binomial
df.noout$not_gen <- df.noout$num_queries - df.noout$num_gen
glm.obj <- glm(cbind(df.noout$num_gen,df.noout$not_gen)~city+gender,
data=df.noout,family=binomial('logit'))
summary(glm.obj)
glm.obj <- glm(cbind(df.noout$num_gen,df.noout$not_gen)~city+num_queries+num_queries_unique+avg_len+gender,
data=df.noout,family=binomial('logit'))
summary(glm.obj)
plot(glm.obj)
row.names(df.noout) <- df.noout$name
glm(as.matrix(df[,c("num_gen","not_gen")])~city+num_queries+num_queries_unique+avg_len+gender,
df=df.noout,family=binomial('logit'))
####FINAL MODEL HERE: BINOMIAL LOGISITIC REGRESSION
df.noout$not_gen <- df.noout$num_queries - df.noout$num_gen
#row.names(df.noout) <- df.noout$name
glm(as.matrix(df[,c("num_gen","not_gen")])~city+num_queries+num_queries_unique+avg_len+gender,
df=df.noout,family=binomial('logit'))
row.names(df.noout) <- df.noout$name
glm.obj <- glm(cbind(df.noout$num_gen,df.noout$not_gen)~city+num_queries+num_queries_unique+avg_len+gender,
data=df.noout,family=binomial('logit'))
plot(glm.obj)
summary(glm.obj)
#model with gender
glm.obj <- glm(cbind(df.noout$num_gen,df.noout$not_gen)~city+num_queries+num_queries_unique+avg_len+gender,
data=df.noout,family=binomial('logit'))
glm.obj2
#model without gender
glm.obj2 <- glm(cbind(df.noout$num_gen,df.noout$not_gen) ~ city+num_queries+
num_queries_unique+avg_len,
data=df.noout,family=binomial('logit'))
#######
AIC(glm.obj)
AIC(glm.obj2)
BIC(glm.obj)
BIC(glm.obj2)
